{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNakBbN1JoIKzZ4FMqMsxt0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# FRUITS DISEASE CLASSIFICATION"],"metadata":{"id":"3db07JGlaUT4"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n"],"metadata":{"id":"X5Usu3h8dKRj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["First downlaod the dataset"],"metadata":{"id":"uDJAw0XabbBI"}},{"cell_type":"code","source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"ateebnoone/fruits-dataset-for-fruit-disease-classification\")\n","\n","print(\"Path to dataset files:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Y4o2KekbTOy","executionInfo":{"status":"ok","timestamp":1764836218090,"user_tz":-60,"elapsed":107246,"user":{"displayName":"Youssef Moustaid","userId":"15234993318816449149"}},"outputId":"7bdf37e9-248f-4ede-b9e8-f45b843a362c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading from https://www.kaggle.com/api/v1/datasets/download/ateebnoone/fruits-dataset-for-fruit-disease-classification?dataset_version_number=2...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4.38G/4.38G [00:45<00:00, 104MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting files...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Path to dataset files: /root/.cache/kagglehub/datasets/ateebnoone/fruits-dataset-for-fruit-disease-classification/versions/2\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"QuVx5_EPbfDT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. Data & Model prep\n"],"metadata":{"id":"lm1YzhOBc95E"}},{"cell_type":"code","source":["# Hyper params\n","BATCH_SIZE = 20\n","IMAGE_SIZE = (224, 224)\n","EPOCHS = 20\n","\n","DATA_DIR = path\n","TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n","VALID_DIR = os.path.join(DATA_DIR, 'validation')\n","TEST_DIR = os.path.join(DATA_DIR, 'test')"],"metadata":{"id":"2RXv1vScc6kA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load the data with ImageDataGenerator to load images , resize them, and apply basic data augmentation(rotaiton, flips...) to improve the model's robustness.\n","# Rescale to [0, 1]\n","train_datagen = ImageDataGenerator(\n","    rescale = 1./255 ,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","# no augmentation for validaiton\n","valid_datagen = ImageDataGenerator(\n","    rescale = 1./255\n",")\n","# load the training data\n","train_generator = train_datagen.flow_from_directory(\n","    TRAIN_DIR,\n","    target_size = IMAGE_SIZE,\n","    batch_size = BATCH_SIZE,\n","    class_mode= 'categorical'\n",")\n","validation_generator = valid_datagen.flow_from_directory(\n","    VALID_DIR,\n","    target_size = IMAGE_SIZE,\n","    batch_size = BATCH_SIZE,\n","    class_mode= 'categorical'\n",")\n","# the number of classes for the final layer\n","NUM_CLASSES = train_generator.num_classes\n","print(f\"Total classes detected : {NUM_CLASSES}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-iHdosbRdGBO","executionInfo":{"status":"ok","timestamp":1764837649125,"user_tz":-60,"elapsed":51,"user":{"displayName":"Youssef Moustaid","userId":"15234993318816449149"}},"outputId":"fbfd9e38-e711-457f-9785-9afd144aeb28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 3115 images belonging to 36 classes.\n","Found 351 images belonging to 36 classes.\n","Total classes detected : 36\n"]}]},{"cell_type":"markdown","source":["![img](https://encrypted-tbn3.gstatic.com/licensed-image?q=tbn:ANd9GcS8ZAQqtM-09H9jSR8hOrkmPZkc9c72vG4q97zfwxLmV5101IvOKMpveIKsUGEGooWe-VT6HqSqqps5EPS0vxdXeJ5tckxYrQwiIAtTxLSFUG_rcwE)"],"metadata":{"id":"g6teDNpJhcNP"}},{"cell_type":"code","source":["# Load base model\n","# Load MobileNetV2 pre-trained on ImageNet, without the top classification layer\n","base_model = tf.keras.applications.MobileNetV2(\n","    input_shape = IMAGE_SIZE + (3,),\n","    include_top = False,\n","    weights = 'imagenet'\n",")\n","# Freeze the base model to prevent weights form being updated during the training\n","base_model.trainable = False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VdMrmOFPhEwY","executionInfo":{"status":"ok","timestamp":1764838190102,"user_tz":-60,"elapsed":2969,"user":{"displayName":"Youssef Moustaid","userId":"15234993318816449149"}},"outputId":"eca81243-d827-4bbb-8f3e-b58a82f5996d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n","\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"]}]},{"cell_type":"code","source":["# Build the custom classififer Head\n","model = Sequential([\n","    base_model,\n","    GlobalAveragePooling2D(),\n","    Dense(128, activation = 'relu'),\n","    Dropout(0.2),# regularization to prevent overfitting\n","    Dense(NUM_CLASSES, activation = 'softmax') # final classification layer\n","])\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"s4WyJidJiN9w","executionInfo":{"status":"ok","timestamp":1764838389252,"user_tz":-60,"elapsed":97,"user":{"displayName":"Youssef Moustaid","userId":"15234993318816449149"}},"outputId":"971c9048-2826-4900-c320-399cd3815525"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n","│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m163,968\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │         \u001b[38;5;34m4,644\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,644</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,426,596\u001b[0m (9.26 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,426,596</span> (9.26 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m168,612\u001b[0m (658.64 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">168,612</span> (658.64 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["model.compile(\n","    optimizer = Adam(learning_rate = 0.0001),\n","    loss = 'categorical_crossentropy',\n","    metrics = ['accuracy']\n",")"],"metadata":{"id":"QOgvXaCLj7I_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Training"],"metadata":{"id":"DfvGF8znkZD-"}},{"cell_type":"code","source":["history = model.fit(\n","    train_generator,\n","    epochs = EPOCHS,\n","    validation_data = validation_generator\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RdCyMhGWkXsK","outputId":"62d4dd9b-f249-44bb-bda3-ca3ffd969e83"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","\u001b[1m  5/156\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:18\u001b[0m 2s/step - accuracy: 0.0528 - loss: 3.9468"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 2s/step - accuracy: 0.0777 - loss: 3.6109 - val_accuracy: 0.4444 - val_loss: 2.5814\n","Epoch 2/20\n","\u001b[1m108/156\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 1s/step - accuracy: 0.3412 - loss: 2.6713"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m142/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m18s\u001b[0m 1s/step - accuracy: 0.3532 - loss: 2.6227"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 1s/step - accuracy: 0.3580 - loss: 2.6017 - val_accuracy: 0.7009 - val_loss: 1.5771\n","Epoch 3/20\n","\u001b[1m 21/156\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:53\u001b[0m 1s/step - accuracy: 0.5734 - loss: 1.7714"]}]},{"cell_type":"code","source":["# save the trained keras model for potential future use\n","model.save('ripness_cnn_model.h5')"],"metadata":{"id":"izmPq7rskwVL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Plotting results"],"metadata":{"id":"4dhg3hxpk6Go"}},{"cell_type":"code","source":["# Plot training history\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(NUM_EPOCHS)\n","\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.title('Training and Validation Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"0og9BaBEk-tT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Convert the Keras model to TFLite"],"metadata":{"id":"T4YK3UhwlFFk"}},{"cell_type":"code","source":["# Initialize the TFLite converter\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","\n","# Apply default optimization (Post-Training Quantization) for smaller size and faster inference\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","\n","# Convert the model\n","tflite_model = converter.convert()\n","\n","# Save the TFLite model file\n","tflite_model_path = 'ripeness_model.tflite'\n","with open(tflite_model_path, 'wb') as f:\n","    f.write(tflite_model)\n","\n","print(f\"TFLite model saved to: {tflite_model_path}\")"],"metadata":{"id":"p5p0YH6ElD3Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Save the Label map\n","since the flutter pap needs a lsit f the class names in the correct order to interpret the model's output"],"metadata":{"id":"yi65BB2plODB"}},{"cell_type":"code","source":["# Get class indices and map them to class names\n","labels = sorted(train_generator.class_indices.items(), key=lambda x: x[1])\n","class_names = [name for name, index in labels]\n","\n","# Save class names to a text file\n","labels_file_path = 'ripeness_labels.txt'\n","with open(labels_file_path, 'w') as f:\n","    f.write('\\n'.join(class_names))\n","\n","print(f\"Label map saved to: {labels_file_path}\")\n","print(\"Final Classes:\", class_names)"],"metadata":{"id":"oxjs7QCLlWQC"},"execution_count":null,"outputs":[]}]}